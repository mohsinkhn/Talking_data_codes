{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abk0005/Competitions/env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.activations import relu\n",
    "\n",
    "import csv\n",
    "from csv import DictReader\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../input/\"\n",
    "train = data_path+'train_sampled.csv'      \n",
    "test = data_path+'test_sampled.csv'\n",
    "submission = 'sub_proba.csv'\n",
    "\n",
    "true_output = data_path+\"output.csv\"\n",
    "\n",
    "corrupt_percentage = 0.10 # corrupt_percentage how much to take????\n",
    "chunk_size = 5000\n",
    "\n",
    "\n",
    "column_dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "train_dim = 50000 \n",
    "test_dim = 50000\n",
    "D = 2 ** 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining train and test data into csv\n",
    "def combine(train, test, chunksize, output):\n",
    "    with open(output, 'w') as r:\n",
    "        r.write('ip,app,device,os,channel,click_time\\n')\n",
    "        for tc, tc1 in zip(pd.read_csv(train, chunksize=chunksize,\n",
    "                                    usecols=[\"ip\", \"app\", \"device\", \"os\", \"channel\", \"click_time\"], \n",
    "                                       dtype = column_dtypes),\n",
    "                       pd.read_csv(test, chunksize= chunksize,\n",
    "                                  usecols=[\"ip\", \"app\", \"device\", \"os\", \"channel\", \"click_time\"], \n",
    "                                   dtype = column_dtypes)):\n",
    "            tc.to_csv(r, chunksize=chunksize, index = False, header = False)\n",
    "            tc1.to_csv(r, chunksize=chunksize, index = False, header = False)\n",
    "combine(train, test, chunk_size, true_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(path, D, output_path):\n",
    "    ''' GENERATOR: Apply hash-trick to the original csv row\n",
    "                   and for simplicity, we one-hot-encode everything\n",
    "\n",
    "        INPUT:\n",
    "            path: path to training or testing file\n",
    "            D: the max index that we can hash to\n",
    "\n",
    "        YIELDS:\n",
    "            ID: id of the instance, mainly useless\n",
    "            x: a list of hashed and one-hot-encoded 'indices'\n",
    "               we only need the index since all values are either 0 or 1\n",
    "            y: y = 1 if we have a click, else we have y = 0\n",
    "    '''\n",
    "    with open(output_path, 'w') as outfile:\n",
    "        outfile.write('ip,app,device,os,channel,date,hour\\n')\n",
    "        for t, row in enumerate(DictReader(open(path))):\n",
    "\n",
    "            # process clicks\n",
    "            y = 0.\n",
    "            if 'is_attributed' in row:\n",
    "                if row['is_attributed'] == '1':\n",
    "                    y = 1.\n",
    "                del row['is_attributed'], row['attributed_time']\n",
    "\n",
    "            try:\n",
    "                click_id = row['click_id']\n",
    "            except:\n",
    "                click_id = ''\n",
    "\n",
    "            # process id\n",
    "            x = []\n",
    "\n",
    "            # Parse hour and date\n",
    "            date, time = row['click_time'].split(' ')\n",
    "            hour = time.split(':')[0]\n",
    "            row['date'] = date\n",
    "            row['hour'] = hour\n",
    "            del row['click_time']\n",
    "\n",
    "            # Add the rest of the features\n",
    "            for k, v in row.items():\n",
    "                x.append(abs(hash('%s_%s'%(k, v))) % D)\n",
    "            outfile.write('%s,%s, %s, %s, %s, %s, %s\\n' % (str(x[0]), str(x[1]), str(x[2]), str(x[3]), str(x[4]), str(x[5]), str(x[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data(true_output, D, data_path+\"train_hashed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare corrupt data\n",
    "def prepare_corrupt_data(path, corrupt_percentage, output_path, \n",
    "                         corrupt_cols = [\"ip\",\"app\", \"device\", 'os','channel', 'date', \"hour\"]):\n",
    "    ## load column\n",
    "    corrupt_data = pd.DataFrame(data = None, columns= corrupt_cols)\n",
    "    for col_name in corrupt_cols:\n",
    "        print(\"reading \", col_name, \"...........\")\n",
    "        col_data = pd.read_csv(path, usecols=[col_name], dtype=column_dtypes)\n",
    "        corrupt_indices = np.random.choice(len(col_data), size=int(col_data.shape[0]*corrupt_percentage))\n",
    "        corrupt_indices2 = np.random.choice(len(col_data), size=int(col_data.shape[0]*corrupt_percentage))\n",
    "        corrupt_col = col_data.copy()\n",
    "        for i,j in zip(corrupt_indices, corrupt_indices2):\n",
    "            corrupt_col.loc[j, col_name] = col_data.loc[i][col_name].values\n",
    "            corrupt_col.loc[i, col_name] = col_data.loc[j][col_name].values\n",
    "        corrupt_data[col_name] = corrupt_col[col_name]\n",
    "    corrupt_data.to_csv(output_path, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading  ip ...........\n",
      "reading  app ...........\n",
      "reading  device ...........\n",
      "reading  os ...........\n",
      "reading  channel ...........\n",
      "reading  date ...........\n",
      "reading  hour ...........\n",
      "CPU times: user 1.67 s, sys: 156 ms, total: 1.83 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## preparing corrupt train data\n",
    "prepare_corrupt_data(data_path+\"train_hashed.csv\", corrupt_percentage=0.15, output_path=data_path+\"corrupt_hashed.csv\")\n",
    "\n",
    "## preparing corrupt test data\n",
    "#prepare_corrupt_data(data_path+\"test_hashed.csv\", corrupt_percentage=0.15, output_path=corrupt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Encoding_in_chunks import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder2 = OneHotEncoder(categorical_columns= [\"ip\",\"app\", \"device\",\"os\", \"channel\", \"date\", \"hour\"])\n",
    "chunked_data = pd.read_csv(data_path+\"train_hashed.csv\", chunksize = chunk_size)\n",
    "encoder2.fit(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 18s, sys: 1min 6s, total: 7min 24s\n",
      "Wall time: 7min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, d in enumerate(pd.read_csv(data_path + \"train_hashed.csv\", chunksize= 1)):\n",
    "    X = encoder2.transform(d)\n",
    "    np.save(\"../input/target_\"+str(i)+\".npy\", X)\n",
    "    \n",
    "for i, d in enumerate(pd.read_csv(data_path + \"corrupt_hashed.csv\", chunksize= 1)):\n",
    "    X = encoder2.transform(d)\n",
    "    np.save(\"../input/target_\"+str(i)+\".npy\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataGenerator import DataGenerator\n",
    "partition = {}\n",
    "partition[\"train\"] = list(np.arange(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 10000\n",
    "\n",
    "# this is our input placeholder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(input_dim, activation='relu')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "(64, 17391) #######\n",
      "   1/1562 [..............................] - ETA: 40:47:13 - loss: 4.9083e-04(64, 17391) #######\n",
      "   2/1562 [..............................] - ETA: 26:57:57 - loss: 4.7931e-04(64, 17391) #######\n",
      "   3/1562 [..............................] - ETA: 20:54:05 - loss: 4.6877e-04(64, 17391) #######\n",
      "   4/1562 [..............................] - ETA: 16:43:20 - loss: 4.5699e-04(64, 17391) #######\n",
      "   5/1562 [..............................] - ETA: 14:44:53 - loss: 4.4533e-04(64, 17391) #######\n",
      "   6/1562 [..............................] - ETA: 12:55:21 - loss: 4.3372e-04(64, 17391) #######\n",
      "   7/1562 [..............................] - ETA: 11:35:46 - loss: 4.2236e-04(64, 17391) #######\n",
      "   8/1562 [..............................] - ETA: 10:40:33 - loss: 4.1235e-04(64, 17391) #######\n",
      "   9/1562 [..............................] - ETA: 9:42:47 - loss: 4.0249e-04 (64, 17391) #######\n",
      "  10/1562 [..............................] - ETA: 8:56:31 - loss: 3.9269e-04(64, 17391) #######\n",
      "  11/1562 [..............................] - ETA: 8:18:46 - loss: 3.8340e-04(64, 17391) #######\n",
      "  12/1562 [..............................] - ETA: 7:47:13 - loss: 3.7476e-04(64, 17391) #######\n",
      "  13/1562 [..............................] - ETA: 7:20:54 - loss: 3.6672e-04(64, 17391) #######\n",
      "  14/1562 [..............................] - ETA: 7:02:53 - loss: 3.5900e-04(64, 17391) #######\n",
      "  15/1562 [..............................] - ETA: 6:43:23 - loss: 3.5210e-04(64, 17391) #######\n",
      "  16/1562 [..............................] - ETA: 6:25:38 - loss: 3.4510e-04(64, 17391) #######\n",
      "  17/1562 [..............................] - ETA: 6:10:08 - loss: 3.3943e-04(64, 17391) #######\n",
      "  18/1562 [..............................] - ETA: 5:56:05 - loss: 3.3411e-04(64, 17391) #######\n",
      "  19/1562 [..............................] - ETA: 5:43:40 - loss: 3.2922e-04(64, 17391) #######\n",
      "  20/1562 [..............................] - ETA: 5:32:51 - loss: 3.2448e-04(64, 17391) #######\n",
      "  21/1562 [..............................] - ETA: 5:22:45 - loss: 3.2096e-04(64, 17391) #######\n",
      "  22/1562 [..............................] - ETA: 119:33:43 - loss: 3.1771e-04(64, 17391) #######\n",
      "  23/1562 [..............................] - ETA: 114:26:06 - loss: 3.1408e-04(64, 17391) #######\n",
      "  24/1562 [..............................] - ETA: 109:55:56 - loss: 3.1079e-04(64, 17391) #######\n",
      "  25/1562 [..............................] - ETA: 105:41:17 - loss: 3.0760e-04(64, 17391) #######\n",
      "  26/1562 [..............................] - ETA: 181:47:47 - loss: 3.0423e-04(64, 17391) #######\n",
      "  27/1562 [..............................] - ETA: 175:02:06 - loss: 3.0096e-04(64, 17391) #######\n",
      "  28/1562 [..............................] - ETA: 173:43:24 - loss: 2.9824e-04(64, 17391) #######\n"
     ]
    }
   ],
   "source": [
    "params = {'dim': input_dim,\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 0,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "input_dim = 17391\n",
    "encoding_dim = 1000\n",
    "training_generator = DataGenerator(partition['train'], labels = None, **params)\n",
    "\n",
    "autoencoder.fit_generator(generator=training_generator,\n",
    "                          steps_per_epoch = int((train_dim+test_dim)/params['batch_size']),\n",
    "                          epochs = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
